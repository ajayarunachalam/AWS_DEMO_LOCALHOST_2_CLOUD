{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize preconfigurations\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu-central-1\n"
     ]
    }
   ],
   "source": [
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::933618615917:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole\n"
     ]
    }
   ],
   "source": [
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-iris-classifier\n",
      "https://s3-eu-central-1.amazonaws.com/sagemaker-iris-classifier\n"
     ]
    }
   ],
   "source": [
    "# Create S3 bucket\n",
    "\n",
    "# This creates a default S3 bucket where we will upload our model.\n",
    "bucket = 'sagemaker-iris-classifier'  #sagemaker.Session().default_bucket()\n",
    "\n",
    "bucket_path = \"https://s3-{}.amazonaws.com/{}\".format(region, bucket)\n",
    "\n",
    "print(bucket)\n",
    "print(bucket_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Install xgboost as it is needed for loading the model from joblib dump file and test it before deployment.\n",
    "Please note that the XGBoost version should be same as the version with which the model was trained locally in laptop.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: - \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - conda-forge/noarch::nbclient==0.5.2=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::matplotlib==3.3.4=py36h5fab9bb_0\n",
      "  - conda-forge/noarch::qdarkstyle==2.8.1=pyhd8ed1ab_2\n",
      "  - conda-forge/linux-64::scikit-image==0.16.2=py36hb3f55d8_0\n",
      "  - conda-forge/noarch::python-language-server==0.36.2=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::widgetsnbextension==3.5.1=py36h5fab9bb_4\n",
      "  - conda-forge/noarch::flake8==3.8.4=py_0\n",
      "  - conda-forge/noarch::ipywidgets==7.6.3=pyhd3deb0d_0\n",
      "  - conda-forge/noarch::typing-extensions==3.7.4.3=0\n",
      "  - conda-forge/noarch::path.py==12.5.0=0\n",
      "  - conda-forge/noarch::dask==2021.2.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::nbformat==5.1.2=pyhd8ed1ab_1\n",
      "  - conda-forge/linux-64::path==15.1.2=py36h5fab9bb_0\n",
      "  - conda-forge/linux-64::nbconvert==6.0.7=py36h5fab9bb_3\n",
      "  - conda-forge/linux-64::distributed==2021.2.0=py36h5fab9bb_0\n",
      "  - conda-forge/noarch::anaconda-client==1.7.2=py_0\n",
      "  - conda-forge/noarch::aioitertools==0.7.1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::matplotlib-base==3.3.4=py36hd391965_0\n",
      "  - conda-forge/linux-64::pluggy==0.13.1=py36h5fab9bb_4\n",
      "  - conda-forge/noarch::black==20.8b1=py_1\n",
      "  - conda-forge/linux-64::blaze==0.11.3=py36_0\n",
      "  - conda-forge/noarch::pyls-spyder==0.3.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::odo==0.5.1=py_1\n",
      "  - conda-forge/linux-64::keyring==22.0.1=py36h5fab9bb_0\n",
      "  - conda-forge/noarch::anaconda-project==0.9.1=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::importlib_metadata==3.7.0=hd8ed1ab_0\n",
      "  - conda-forge/linux-64::jupyter==1.0.0=py36h5fab9bb_6\n",
      "  - conda-forge/noarch::jupyterlab_server==2.3.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::seaborn-base==0.11.1=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::imageio==2.9.0=py_0\n",
      "  - conda-forge/noarch::numpydoc==1.1.0=py_1\n",
      "  - conda-forge/linux-64::yarl==1.6.3=py36h8f6f2f9_1\n",
      "  - conda-forge/noarch::jsonschema==3.2.0=py_2\n",
      "  - conda-forge/noarch::flask==1.1.2=pyh9f0ad1d_0\n",
      "  - conda-forge/noarch::seaborn==0.11.1=hd8ed1ab_1\n",
      "  - conda-forge/noarch::helpdev==0.7.1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::nb_conda==2.2.1=py36h5fab9bb_4\n",
      "  - conda-forge/noarch::nbclassic==0.2.6=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::sphinx==3.5.1=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::jupyterlab_launcher==0.13.1=py_2\n",
      "  - conda-forge/linux-64::spyder==4.2.0=py36h5fab9bb_0\n",
      "  - conda-forge/linux-64::importlib-metadata==3.7.0=py36h5fab9bb_0\n",
      "  - conda-forge/linux-64::pytest==6.2.2=py36h5fab9bb_0\n",
      "  - conda-forge/noarch::pyls-black==0.4.6=pyh9f0ad1d_0\n",
      "done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.4\n",
      "  latest version: 4.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/python3\n",
      "\n",
      "  added / updated specs:\n",
      "    - xgboost==0.90\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _py-xgboost-mutex-2.0      |            cpu_0           8 KB  conda-forge\n",
      "    anyio-3.3.2                |   py36h5fab9bb_0         147 KB  conda-forge\n",
      "    astroid-2.7.3              |   py36h5fab9bb_0         330 KB  conda-forge\n",
      "    bokeh-2.3.3                |   py36h5fab9bb_0         8.3 MB  conda-forge\n",
      "    charset-normalizer-2.0.10  |     pyhd8ed1ab_0          34 KB  conda-forge\n",
      "    colorama-0.4.4             |     pyh9f0ad1d_0          18 KB  conda-forge\n",
      "    dataclasses-0.8            |     pyh787bdff_2          22 KB  conda-forge\n",
      "    docutils-0.16              |   py36h5fab9bb_3         738 KB  conda-forge\n",
      "    flask-cors-3.0.10          |     pyhd8ed1ab_0          15 KB  conda-forge\n",
      "    fsspec-2021.11.1           |     pyhd8ed1ab_0          91 KB  conda-forge\n",
      "    jupyter_console-5.2.0      |           py36_1          34 KB  conda-forge\n",
      "    jupyter_server-1.13.1      |     pyhd8ed1ab_0         266 KB  conda-forge\n",
      "    libxgboost-0.90            |       he1b5a44_4         2.4 MB  conda-forge\n",
      "    notebook-6.3.0             |   py36h5fab9bb_0         6.3 MB  conda-forge\n",
      "    pillow-8.2.0               |   py36ha6010c0_1         688 KB  conda-forge\n",
      "    platformdirs-2.3.0         |     pyhd8ed1ab_0          14 KB  conda-forge\n",
      "    py-xgboost-0.90            |           py36_4          73 KB  conda-forge\n",
      "    pylint-2.10.2              |     pyhd8ed1ab_0         255 KB  conda-forge\n",
      "    requests-2.27.1            |     pyhd8ed1ab_0          53 KB  conda-forge\n",
      "    typing_extensions-3.7.4.3  |             py_0          25 KB  conda-forge\n",
      "    urllib3-1.26.8             |     pyhd8ed1ab_1         100 KB  conda-forge\n",
      "    websocket-client-1.2.3     |     pyhd8ed1ab_0          41 KB  conda-forge\n",
      "    werkzeug-2.0.1             |     pyhd8ed1ab_0         219 KB  conda-forge\n",
      "    xgboost-0.90               |   py36he1b5a44_4          11 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        20.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _py-xgboost-mutex  conda-forge/linux-64::_py-xgboost-mutex-2.0-cpu_0\n",
      "  anyio              conda-forge/linux-64::anyio-3.3.2-py36h5fab9bb_0\n",
      "  astroid            conda-forge/linux-64::astroid-2.7.3-py36h5fab9bb_0\n",
      "  babel              conda-forge/noarch::babel-2.9.1-pyh44b312d_0\n",
      "  bleach             conda-forge/noarch::bleach-4.1.0-pyhd8ed1ab_0\n",
      "  bokeh              conda-forge/linux-64::bokeh-2.3.3-py36h5fab9bb_0\n",
      "  charset-normalizer conda-forge/noarch::charset-normalizer-2.0.10-pyhd8ed1ab_0\n",
      "  colorama           conda-forge/noarch::colorama-0.4.4-pyh9f0ad1d_0\n",
      "  dask-core          conda-forge/noarch::dask-core-2021.2.0-pyhd8ed1ab_0\n",
      "  dataclasses        conda-forge/noarch::dataclasses-0.8-pyh787bdff_2\n",
      "  docutils           conda-forge/linux-64::docutils-0.16-py36h5fab9bb_3\n",
      "  flask-cors         conda-forge/noarch::flask-cors-3.0.10-pyhd8ed1ab_0\n",
      "  fsspec             conda-forge/noarch::fsspec-2021.11.1-pyhd8ed1ab_0\n",
      "  jupyter_console    conda-forge/linux-64::jupyter_console-5.2.0-py36_1\n",
      "  jupyter_server     conda-forge/noarch::jupyter_server-1.13.1-pyhd8ed1ab_0\n",
      "  libxgboost         conda-forge/linux-64::libxgboost-0.90-he1b5a44_4\n",
      "  nest-asyncio       conda-forge/noarch::nest-asyncio-1.5.4-pyhd8ed1ab_0\n",
      "  notebook           conda-forge/linux-64::notebook-6.3.0-py36h5fab9bb_0\n",
      "  openjpeg           conda-forge/linux-64::openjpeg-2.4.0-hb52868f_1\n",
      "  packaging          conda-forge/noarch::packaging-21.3-pyhd8ed1ab_0\n",
      "  pillow             conda-forge/linux-64::pillow-8.2.0-py36ha6010c0_1\n",
      "  platformdirs       conda-forge/noarch::platformdirs-2.3.0-pyhd8ed1ab_0\n",
      "  py-xgboost         conda-forge/linux-64::py-xgboost-0.90-py36_4\n",
      "  pylint             conda-forge/noarch::pylint-2.10.2-pyhd8ed1ab_0\n",
      "  requests           conda-forge/noarch::requests-2.27.1-pyhd8ed1ab_0\n",
      "  send2trash         conda-forge/noarch::send2trash-1.8.0-pyhd8ed1ab_0\n",
      "  typing_extensions  conda-forge/noarch::typing_extensions-3.7.4.3-py_0\n",
      "  urllib3            conda-forge/noarch::urllib3-1.26.8-pyhd8ed1ab_1\n",
      "  websocket-client   conda-forge/noarch::websocket-client-1.2.3-pyhd8ed1ab_0\n",
      "  werkzeug           conda-forge/noarch::werkzeug-2.0.1-pyhd8ed1ab_0\n",
      "  xgboost            conda-forge/linux-64::xgboost-0.90-py36he1b5a44_4\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "fsspec-2021.11.1     | 91 KB     | ##################################### | 100% \n",
      "py-xgboost-0.90      | 73 KB     | ##################################### | 100% \n",
      "websocket-client-1.2 | 41 KB     | ##################################### | 100% \n",
      "typing_extensions-3. | 25 KB     | ##################################### | 100% \n",
      "libxgboost-0.90      | 2.4 MB    | ##################################### | 100% \n",
      "jupyter_server-1.13. | 266 KB    | ##################################### | 100% \n",
      "pylint-2.10.2        | 255 KB    | ##################################### | 100% \n",
      "jupyter_console-5.2. | 34 KB     | ##################################### | 100% \n",
      "astroid-2.7.3        | 330 KB    | ##################################### | 100% \n",
      "flask-cors-3.0.10    | 15 KB     | ##################################### | 100% \n",
      "xgboost-0.90         | 11 KB     | ##################################### | 100% \n",
      "anyio-3.3.2          | 147 KB    | ##################################### | 100% \n",
      "docutils-0.16        | 738 KB    | ##################################### | 100% \n",
      "charset-normalizer-2 | 34 KB     | ##################################### | 100% \n",
      "urllib3-1.26.8       | 100 KB    | ##################################### | 100% \n",
      "werkzeug-2.0.1       | 219 KB    | ##################################### | 100% \n",
      "dataclasses-0.8      | 22 KB     | ##################################### | 100% \n",
      "notebook-6.3.0       | 6.3 MB    | ##################################### | 100% \n",
      "bokeh-2.3.3          | 8.3 MB    | ##################################### | 100% \n",
      "platformdirs-2.3.0   | 14 KB     | ##################################### | 100% \n",
      "pillow-8.2.0         | 688 KB    | ##################################### | 100% \n",
      "requests-2.27.1      | 53 KB     | ##################################### | 100% \n",
      "colorama-0.4.4       | 18 KB     | ##################################### | 100% \n",
      "_py-xgboost-mutex-2. | 8 KB      | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -y -c conda-forge xgboost==0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"DEMO-local-xgboost-classifier-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model, and test it before deployment\n",
    "import joblib\n",
    "import xgboost\n",
    "\n",
    "mymodel = joblib.load(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.4 3.  4.5 1.5]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.1 3.3 1.7 0.5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 2, 0], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import test file\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file_name = (\n",
    "    \"test_point_classifier_sample.csv\"  # customize to your test file, will be 'mnist.single.test' if use data above\n",
    ")\n",
    "\n",
    "with open(file_name, \"r\") as f:\n",
    "    mypayload = np.loadtxt(f, delimiter=\",\")\n",
    "    \n",
    "print(mypayload)    \n",
    "\n",
    "mymodel.predict(mypayload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tar.gz model file as this is the format required by Sagemaker for deployment.\n",
    "mymodel._Booster.save_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEMO-local-xgboost-classifier-model\r\n"
     ]
    }
   ],
   "source": [
    "!tar czvf model.tar.gz $model_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker/DEMO-xgboost-classifier-byo/DEMO-local-xgboost-classifier-model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Upload the pre-trained model to S3\n",
    "\n",
    "#### prefix in S3\n",
    "prefix = \"sagemaker/DEMO-xgboost-classifier-byo\"\n",
    "\n",
    "fObj = open(\"model.tar.gz\", \"rb\")\n",
    "key = os.path.join(prefix, model_file_name, \"model.tar.gz\")\n",
    "print(key)\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(key).upload_fileobj(fObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up hosting for the model\n",
    "'''\n",
    "Import model into hosting\n",
    "This involves creating a SageMaker model from the model file previously uploaded to S3.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "# Create a Sagemaker model\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "#### Get the built-in xgboost container image in Sagemaker to host our model\n",
    "container = get_image_uri(boto3.Session().region_name, \"xgboost\", \"0.90-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3-eu-central-1.amazonaws.com/sagemaker-iris-classifier/sagemaker/DEMO-xgboost-classifier-byo/DEMO-local-xgboost-classifier-model/model.tar.gz\n",
      "arn:aws:sagemaker:eu-central-1:933618615917:model/demo-local-xgboost-classifier-model2022-01-10-09-19-46\n",
      "CPU times: user 80.6 ms, sys: 2.19 ms, total: 82.8 ms\n",
      "Wall time: 461 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_name = model_file_name + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "model_url = \"https://s3-{}.amazonaws.com/{}/{}\".format(region, bucket, key)\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "print(model_url)\n",
    "\n",
    "primary_container = {\n",
    "    \"Image\": container,\n",
    "    \"ModelDataUrl\": model_url,\n",
    "}\n",
    "\n",
    "create_model_response2 = sm_client.create_model(\n",
    "    ModelName=model_name, ExecutionRoleArn=role, PrimaryContainer=primary_container\n",
    ")\n",
    "\n",
    "print(create_model_response2[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEMO-XGBoost-Classifier-EndpointConfig-2022-01-10-09-19-55\n",
      "Endpoint Config Arn: arn:aws:sagemaker:eu-central-1:933618615917:endpoint-config/demo-xgboost-classifier-endpointconfig-2022-01-10-09-19-55\n"
     ]
    }
   ],
   "source": [
    "# Create endpoint configuration\n",
    "'''\n",
    "Create an endpoint configuration, that describes the distribution of traffic across the models, whether split, shadowed, or sampled in some way. \n",
    "In addition, the endpoint configuration describes the instance type required for model deployment.\n",
    "'''\n",
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_config_name = \"DEMO-XGBoost-Classifier-EndpointConfig-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "print(endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.m4.xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"ModelName\": model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEMO-XGBoost-Classifier-Endpoint-2022-01-10-09-20-25\n",
      "arn:aws:sagemaker:eu-central-1:933618615917:endpoint/demo-xgboost-classifier-endpoint-2022-01-10-09-20-25\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:eu-central-1:933618615917:endpoint/demo-xgboost-classifier-endpoint-2022-01-10-09-20-25\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "# Create endpoint\n",
    "'''\n",
    "Lastly, you create the endpoint that serves up the model, through specifying the name and configuration defined above. \n",
    "The end result is an endpoint that can be validated and incorporated into production applications.\n",
    "'''\n",
    "import time\n",
    "endpoint_name = \"DEMO-XGBoost-Classifier-Endpoint-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(create_endpoint_response[\"EndpointArn\"])\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model for use\n",
    "'''\n",
    "Now you can obtain the endpoint from the client library using the result from previous operations \n",
    "and generate classifications from the model using that endpoint.\n",
    "'''\n",
    "runtime_client = boto3.client(\"runtime.sagemaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload :\n",
      "\n",
      "5.400000000000000355e+00,3.000000000000000000e+00,4.500000000000000000e+00,1.500000000000000000e+00\n",
      "5.599999999999999645e+00,3.000000000000000000e+00,4.099999999999999645e+00,1.300000000000000044e+00\n",
      "6.299999999999999822e+00,2.799999999999999822e+00,5.099999999999999645e+00,1.500000000000000000e+00\n",
      "6.000000000000000000e+00,3.000000000000000000e+00,4.799999999999999822e+00,1.800000000000000044e+00\n",
      "5.099999999999999645e+00,3.299999999999999822e+00,1.699999999999999956e+00,5.000000000000000000e-01\n",
      "\n",
      "Results :\n",
      "\n",
      "\n",
      "\n",
      "Predicted Class Probabilities: [0.056180261075496674, 0.887361466884613, 0.056458208709955215],[0.056180261075496674, 0.887361466884613, 0.056458208709955215],[0.08053058385848999, 0.8385403752326965, 0.08092901110649109],[0.12150664627552032, 0.27632802724838257, 0.6021653413772583],[0.8827555775642395, 0.0601295568048954, 0.05711490288376808].\n",
      "CPU times: user 924 Âµs, sys: 4.27 ms, total: 5.19 ms\n",
      "Wall time: 17 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "\n",
    "file_name = (\n",
    "    \"test_point_classifier_sample.csv\"  # customize to your test file\n",
    ")\n",
    "\n",
    "with open(file_name, \"r\") as f:\n",
    "    payload = f.read().strip()\n",
    "    \n",
    "    \n",
    "print(\"Payload :\\n\")\n",
    "\n",
    "print(payload)\n",
    "print()\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"text/csv\", Body=payload\n",
    ")\n",
    "\n",
    "##print(response)\n",
    "\n",
    "print(\"Results :\\n\")\n",
    "print()\n",
    "\n",
    "result = response[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "# Unpack response\n",
    "print(\"\\nPredicted Class Probabilities: {}.\".format(result))\n",
    "classes = ['Setosa', 'Versicolor', 'Virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Endpoint\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "runtime_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
